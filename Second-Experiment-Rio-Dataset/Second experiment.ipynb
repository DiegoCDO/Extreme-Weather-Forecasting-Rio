{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb463aae",
   "metadata": {},
   "source": [
    "# Applying XGBoost to guess multiple points in the future\n",
    "\n",
    "In this Jupyter Notebook, we use XGBoost to predict M points in the future from the N previous points. The input include all the features of the São Cristóvão station and the output include M points with one of the cumulative rain feature (that can be chagned manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a982e0f",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "In this section we prepare some tool, modules, and preprocessing on the dataset to run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e340e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from threading import Thread\n",
    "from threading import Lock\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1c7b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if \"data/output\" not in os.getcwd():\n",
    "    os.chdir(\"data/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bc910",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "In this section the dataset is loaded and unified as one pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4242699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "station = \"sao_cristovao\"\n",
    "sources = ['AlertaRio_DadosPluv', 'AlertaRio_DadosMet']\n",
    "data[station] = {}\n",
    "source = sources[0]\n",
    "data[station][source] = data[station][source] = pd.read_csv(source + \"/full/\" + station + \".csv\", sep=',')\n",
    "source = sources[1]\n",
    "data[station][source] = data[station][source] = pd.read_csv(source + \"/full/\" + station + \".csv\", sep=',')\n",
    "\n",
    "for source in sources:\n",
    "    data[station][source]['datetime'] = pd.to_datetime(data[station][source]['Dia'] + data[station][source]['Hora'], format='%d/%m/%Y%H:%M:%S')\n",
    "    data[station][source].set_index('datetime', inplace=True)\n",
    "    data[station][source] = data[station][source].asfreq(\"15T\")[\"2000\":\"2023-05-18 02:00:00\"]\n",
    "data[station][sources[1]].drop(columns=[\"Chuva\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce7e070",
   "metadata": {},
   "source": [
    "Checking for the same range of date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a840dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sao_cristovao\"][\"AlertaRio_DadosPluv\"].iloc[0].name == \\\n",
    "data[\"sao_cristovao\"][\"AlertaRio_DadosMet\"].iloc[0].name, \\\n",
    "\\\n",
    "data[\"sao_cristovao\"][\"AlertaRio_DadosPluv\"].iloc[-1].name == \\\n",
    "data[\"sao_cristovao\"][\"AlertaRio_DadosMet\"].iloc[-1].name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c7d1e8",
   "metadata": {},
   "source": [
    "Concatenate the 2 datasets and remove useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7620664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Dia', 'Hora']\n",
    "data_features = pd.concat([data[\"sao_cristovao\"][\"AlertaRio_DadosPluv\"].drop(columns=drop_list), data[\"sao_cristovao\"][\"AlertaRio_DadosMet\"].drop(columns=drop_list)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0804bf87",
   "metadata": {},
   "source": [
    "### Features extraction\n",
    "\n",
    "Extract the features from the date to run XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52917ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_features(df):\n",
    "    out = df.copy()\n",
    "    out[\"min\"] = out.index.minute\n",
    "    out[\"hour\"] = out.index.hour\n",
    "    out[\"day\"] = out.index.day\n",
    "    out[\"month\"] = out.index.month\n",
    "    out[\"year\"] = out.index.year\n",
    "    \n",
    "    out['quarter'] = out.index.quarter\n",
    "    out['dayofyear'] = out.index.dayofyear\n",
    "    out['dayofmonth'] = out.index.day\n",
    "    \n",
    "    out['weekofyear'] = out.index.isocalendar().week.astype(np.int64)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b975a5",
   "metadata": {},
   "source": [
    "Defining the number of points to predict in the future and choose the target (cumulative rain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a78443",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_target = 96  # Number of shifted columns\n",
    "target = '04h'\n",
    "\n",
    "targets = [target]\n",
    "\n",
    "# Create shifted columns\n",
    "for i in range(1, N_target + 1):\n",
    "    targets.append(f'Fut_{target}{i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67be4760",
   "metadata": {},
   "source": [
    "### Missing data\n",
    "\n",
    "Dealing with missing data using linear regression and using only the data from 2002 to today as the station don't provide all the features in it's first 2 years of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ddaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features['2002':][target].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d55cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_features[target] = data_features[target].interpolate(method='linear')\n",
    "\n",
    "data_features = data_features['2010':]\n",
    "\n",
    "data_features[target].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a7d47",
   "metadata": {},
   "source": [
    "### Group points\n",
    "\n",
    "In this section the data from the previous points and the target of the next M points is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307cc5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_colums_names(column_names, N):\n",
    "    column_names = list(column_names)\n",
    "    names = []\n",
    "    for i in range(N, 0, -1):\n",
    "        for name in column_names:\n",
    "            names.append(name + str(i))\n",
    "    names.extend(column_names)\n",
    "    return names\n",
    "\n",
    "data_features.reset_index(inplace=True)\n",
    "all_available_features = list(data_features.columns)\n",
    "\n",
    "N = 15 # Number of points to predict future\n",
    "data_multiple = data_features.copy()\n",
    "\n",
    "for i in range(1, N):\n",
    "    data_multiple = pd.concat([data_multiple.iloc[:-1].reset_index(drop=True), data_features.iloc[i:].reset_index(drop=True)], axis=1)\n",
    "\n",
    "data_multiple = pd.concat([data_multiple.iloc[:-1].reset_index(drop=True), data_features.iloc[N:].reset_index(drop=True)], axis=1)\n",
    "\n",
    "data_multiple.columns = get_colums_names(data_features.columns, N)\n",
    "\n",
    "data_multiple.drop(columns=[\"datetime\" + str(i) for i in range(1, N + 1)], inplace=True)\n",
    "data_multiple.set_index(\"datetime\", inplace=True)\n",
    "\n",
    "# Create shifted columns\n",
    "for i in range(1, N_target + 1):\n",
    "    col_name = targets[i]\n",
    "    data_multiple[col_name] = data_multiple[target].shift(-i)\n",
    "\n",
    "# data_multiple = get_date_features(data_multiple)\n",
    "data_multiple = data_multiple[:-N_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a362187",
   "metadata": {},
   "source": [
    "Checking if there is missing data in the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639e380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_multiple[targets].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a80cb",
   "metadata": {},
   "source": [
    "### Partitionning\n",
    "\n",
    "Partitionning the dataset as below:\n",
    "\n",
    "4 partitions:\n",
    "\n",
    "- Training Extreme Event\n",
    "- Training Usual Weather\n",
    "- Testing Extreme Event\n",
    "- Testing Usual Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used if you want to use MAPE as score metric function\n",
    "# data_multiple += 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50c0bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Timedelta(\"15T\")\n",
    "\n",
    "def date_to_index(date, starting_date):\n",
    "    return (pd.to_datetime(date, format='%d/%m/%Y') - starting_date) / t\n",
    "\n",
    "def index_to_date(index, starting_date):\n",
    "    return starting_date + index * t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a271d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_events_dates = pd.to_datetime(pd.DataFrame([\"19/03/2000\",\n",
    "                                                    \"11/06/2006\",\n",
    "                                                    \"04/04/2010\",\n",
    "                                                    \"06/04/2010\",\n",
    "                                                    \"25/04/2011\",\n",
    "                                                    \"26/04/2011\",\n",
    "                                                    \"12/03/2016\",\n",
    "                                                    \"14/02/2018\",\n",
    "                                                    \"15/02/2018\",\n",
    "                                                    \"08/04/2019\",\n",
    "                                                    \"09/04/2019\"])[0], format='%d/%m/%Y')\n",
    "extreme_events_index = [date_to_index(date, data_multiple.index.min()) for date in extreme_events_dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff81cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extreme_events_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818613c1",
   "metadata": {},
   "source": [
    "Creating the Extreme events partition by splitting 4 extreme event in training and 2 in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c5758",
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_event_list = [(605300, 605700),\n",
    "                        (565240, 565440),\n",
    "                        (497620, 497820),\n",
    "                        (326480, 326700),\n",
    "                        (289500, 289850),\n",
    "                        (155710, 155900)]\n",
    "\n",
    "extreme_event_list = [(int(i) - 150, int(i) + 150) for i in extreme_events_index if i > 0]\n",
    "\n",
    "extreme_sep = 4\n",
    "\n",
    "extreme_events = pd.DataFrame()\n",
    "for i, j in extreme_event_list:\n",
    "    extreme_events = pd.concat([extreme_events, data_multiple[i:j]])\n",
    "    \n",
    "extreme_training = pd.DataFrame()\n",
    "for i, j in extreme_event_list[:extreme_sep]:\n",
    "    extreme_training = pd.concat([extreme_training, data_multiple[i:j]])\n",
    "    \n",
    "extreme_testing = pd.DataFrame()\n",
    "for i, j in extreme_event_list[extreme_sep:]:\n",
    "    extreme_testing = pd.concat([extreme_testing, data_multiple[i:j]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22020961",
   "metadata": {},
   "source": [
    "Creating usual weather partition (picking random segment and check that they aren't extreme event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bbb606",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(3, 30, (2, 5 ))\n",
    "\n",
    "nb_seg_training = 100\n",
    "nb_seg_testing = 400\n",
    "len_seg = 10\n",
    "\n",
    "random_training = pd.DataFrame()\n",
    "\n",
    "for _ in range(nb_seg_training): # Number of random segment\n",
    "    while True:\n",
    "        ran = np.random.randint(0, len(data_multiple))\n",
    "        ok = True\n",
    "        for i, j in extreme_event_list:\n",
    "            if i in range(ran, ran + len_seg):\n",
    "                ok = False\n",
    "            if j in range(ran, ran + len_seg):\n",
    "                ok = False\n",
    "        if ok:\n",
    "            break\n",
    "    random_training = pd.concat([random_training, data_multiple[ran:ran + len_seg]])\n",
    "\n",
    "random_testing = pd.DataFrame()\n",
    "\n",
    "for _ in range(nb_seg_testing): # Number of random segment\n",
    "    while True:\n",
    "        ran = np.random.randint(0, len(data_multiple))\n",
    "        ok = True\n",
    "        for i, j in extreme_event_list:\n",
    "            if i in range(ran, ran + len_seg):\n",
    "                ok = False\n",
    "            if j in range(ran, ran + len_seg):\n",
    "                ok = False\n",
    "        if ok:\n",
    "            break\n",
    "    random_testing = pd.concat([random_testing, data_multiple[ran:ran + len_seg]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ab11e",
   "metadata": {},
   "source": [
    "## Training and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb8b10",
   "metadata": {},
   "source": [
    "Defining the training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c4fd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_features = data_multiple.columns\n",
    "\n",
    "training_features_list = ['15min', '01h', '04h', '24h', '96h', 'DirVento',\n",
    "       'VelVento', 'Temperatura', 'Pressao', 'Umidade', 'hour', 'day', 'month',\n",
    "       'year', 'quarter', 'dayofyear', 'dayofmonth', 'weekofyear']\n",
    "\n",
    "# training_features_list = ['15min', '01h', '04h', '24h', '96h', 'DirVento',\n",
    "#        'VelVento', 'Temperatura', 'Pressao', 'Umidade']\n",
    "\n",
    "# Features to avoid for each point (If a feature like 15min is in this list,\n",
    "# then training feature will contains 15min1, ..., 15minN but not 15min)\n",
    "avoid_features = ['datetime', '15min', '01h', '04h', '24h', '96h', 'DirVento',\n",
    "       'VelVento', 'Temperatura', 'Pressao', 'Umidade']\n",
    "\n",
    "def is_training_feature(feature, training_features, avoid_features):\n",
    "    for training_feature in training_features:\n",
    "        if feature not in avoid_features and \\\n",
    "        training_feature == feature[:len(training_feature)] and \\\n",
    "        (feature[len(training_feature):].isnumeric() or feature[len(training_feature):] == \"\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "training_features = list(filter(lambda x : is_training_feature(x, training_features_list, avoid_features), all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5213029",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846e4b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"n_estimators\" : 200,\n",
    "    \"learning_rate\" : 0.05\n",
    "}\n",
    "\n",
    "reg_list = []\n",
    "\n",
    "# This condition allow to use more than one metric\n",
    "# If you don't have a Nvidia GPU remove the tree_method argument in the XGBoost regressor\n",
    "if True:\n",
    "    \"\"\"One metric\"\"\"\n",
    "    reg = xgb.XGBRegressor(tree_method=\"gpu_hist\",\n",
    "                           **args)\n",
    "    reg_list.append(reg)\n",
    "else:\n",
    "    \"\"\"Multiple metrics\"\"\"\n",
    "    reg_both = xgb.XGBRegressor(tree_method=\"gpu_hist\",\n",
    "                           eval_metric=['rmse', 'mape'],\n",
    "                           **args)\n",
    "    reg_rmse = xgb.XGBRegressor(tree_method=\"gpu_hist\",\n",
    "                           eval_metric=['rmse'],\n",
    "                           **args)\n",
    "    reg_mape = xgb.XGBRegressor(tree_method=\"gpu_hist\",\n",
    "                           eval_metric=['mape'],\n",
    "                           **args)\n",
    "    reg_list.append(reg_both)\n",
    "    reg_list.append(reg_rmse)\n",
    "    reg_list.append(reg_mape)\n",
    "\n",
    "for reg in reg_list:\n",
    "    print(\"\\nstarting with :\", reg.eval_metric)\n",
    "    tmp = time()\n",
    "    \n",
    "    if True:\n",
    "        reg.fit(pd.concat([extreme_training[training_features], random_training[training_features]]), # Training partition = extreme event + usual weather\n",
    "                pd.concat([extreme_training[targets], random_training[targets]]), # Training targets\n",
    "                eval_set=[(extreme_training[training_features], extreme_training[targets]), # Evaluating set = all partition\n",
    "                         (extreme_testing[training_features], extreme_testing[targets]),\n",
    "                          (random_training[training_features], random_training[targets]),\n",
    "                         (random_testing[training_features], random_testing[targets])],\n",
    "                verbose=20)\n",
    "    else:\n",
    "        reg.set_params(learning_rate=1)\n",
    "        \n",
    "        print(\"Training with random data\")\n",
    "        reg.fit(random_training[training_features], random_training[targets],\n",
    "                eval_set=[(random_training[training_features], random_training[targets]),\n",
    "                         (random_testing[training_features], random_testing[targets])],\n",
    "                verbose=50)\n",
    "\n",
    "        reg.set_params(learning_rate=0.01)\n",
    "        print(\"Training with Extreme event\")\n",
    "        reg.fit(extreme_training[training_features], extreme_training[targets],\n",
    "                eval_set=[(extreme_training[training_features], extreme_training[targets]),\n",
    "                         (extreme_testing[training_features], extreme_testing[targets])],\n",
    "                verbose=50)\n",
    "    print(\"Time :\", time() - tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ac00b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extreme_events_index # Usefull to manually ajust the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298928f",
   "metadata": {},
   "source": [
    "Plotting using a manual starting index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c05f55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_index = 324880\n",
    "    \n",
    "print(start_index)\n",
    "print(index_to_date(start_index, data_multiple.index.min())) # Print the date of the start index\n",
    "\n",
    "for reg in reg_list:\n",
    "    print(\"\\nstarting with :\", reg.eval_metric)\n",
    "\n",
    "    preds_test = reg.predict(data_multiple[training_features][start_index:start_index + 1])\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    grid = plt.GridSpec(2, 2, figure=fig)\n",
    "\n",
    "    # Plotting the first subplot data_features(top)\n",
    "    ax1 = fig.add_subplot(grid[0, :])\n",
    "    ax1.set_title('Testing Data/predicted value')\n",
    "    ax1.plot(data_multiple[start_index:start_index + N_target + 1][target], color=\"blue\")\n",
    "    ax1.plot(data_multiple.index[start_index:start_index + N_target + 1], preds_test[0], alpha=0.7, color=\"red\")\n",
    "    ax1.legend(['Testing Set', 'Prediction'])\n",
    "\n",
    "    # Plotting the second subplot (bottom left)\n",
    "    ax2 = fig.add_subplot(grid[1, 0], sharey=ax1)\n",
    "    ax2.set_title('Testing Data')\n",
    "    ax2.plot(data_multiple[start_index:start_index + N_target + 1][target], color=\"blue\")\n",
    "    ax2.legend(['Testing Set'])\n",
    "\n",
    "    # Plotting the third subplot (bottom right)\n",
    "    ax3 = fig.add_subplot(grid[1, 1], sharey=ax1)\n",
    "    ax3.set_title('Prediction value')\n",
    "    ax3.plot(data_multiple.index[start_index:start_index + N_target + 1], preds_test[0], color=\"red\")\n",
    "    ax3.legend(['Prediction'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1205a8c4",
   "metadata": {},
   "source": [
    "Plotting using a random starting index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168b5aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = np.random.randint(0, len(data_multiple))\n",
    "    \n",
    "print(start_index)\n",
    "print(index_to_date(start_index, data_multiple.index.min())) # Print the date of the start index\n",
    "\n",
    "for reg in reg_list:\n",
    "    print(\"\\nstarting with :\", reg.eval_metric)\n",
    "\n",
    "    preds_test = reg.predict(data_multiple[training_features][start_index:start_index + 1])\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    grid = plt.GridSpec(2, 2, figure=fig)\n",
    "\n",
    "    # Plotting the first subplot (top)\n",
    "    ax1 = fig.add_subplot(grid[0, :])\n",
    "    ax1.set_title('Testing Data/predicted value')\n",
    "    ax1.plot(data_multiple[start_index:start_index + N_target + 1][target], color='blue')\n",
    "    ax1.plot(data_multiple.index[start_index:start_index + N_target + 1], preds_test[0], alpha=0.7, color='red')\n",
    "    ax1.legend(['Testing Set', 'Prediction'])\n",
    "\n",
    "    # Plotting the second subplot (bottom left)\n",
    "    ax2 = fig.add_subplot(grid[1, 0], sharey=ax1)\n",
    "    ax2.set_title('Testing Data')\n",
    "    ax2.plot(data_multiple[start_index:start_index + N_target + 1][target], color='blue')\n",
    "    ax2.legend(['Testing Set'])\n",
    "\n",
    "    # Plotting the third subplot (bottom right)\n",
    "    ax3 = fig.add_subplot(grid[1, 1], sharey=ax1)\n",
    "    ax3.set_title('Prediction value')\n",
    "    ax3.plot(data_multiple.index[start_index:start_index + N_target + 1], preds_test[0], color=\"red\")\n",
    "    ax3.legend(['Prediction'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc473dcc",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The model is extremely inconsistant, sometimes the prediction is very good, sometimes the model predict an extreme event when there is no rain, sometime it predict no rain when there is an intense rainfall. And if the model predict an extreme event, 2 points later it can predict nothing as it is highly sensitive.\n",
    "\n",
    "Those experiments show that more research is needed in this field and highlight the difficulty of the process of predicting extreme events.\n",
    "\n",
    "The diffuclty of the experiment can be explain by those following points:\n",
    "\n",
    "- Few amount of data on extreme event\n",
    "- Extreme event characteristics may vary\n",
    "- Not seasonal\n",
    "\n",
    "However, there is hope in improving the results with the following aspect:\n",
    "\n",
    "- Use other features:\n",
    "    - Other meteorological station\n",
    "    - Other data sources such as oceanic probe, radars, etc\n",
    "- Spend more time on training tweaking :\n",
    "    - Hyperparameters\n",
    "    - Training partition\n",
    "    - Cross-validation\n",
    "- Use a better interpolation method on the missing data (A linear interpolation was made)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
