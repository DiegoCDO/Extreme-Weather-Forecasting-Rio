{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c0ae22",
   "metadata": {},
   "source": [
    "# First experiment\n",
    "\n",
    "**Monofeature**\n",
    "\n",
    "In this Jupyter Notebook, an XGBoost regressor is applied to guess the amount of rain that will occur at the next point (15 minute later in our dataset). The dataset used is the São Cristóvão station from AlertaRio's website, here both of the rainfall and meteorological data are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59b4aa1",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "In this section we prepare some tool, modules, and preprocessing on the dataset to run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b572c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from time import time\n",
    "from threading import Thread\n",
    "from threading import Lock\n",
    "\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3688726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if \"data/output\" not in os.getcwd():\n",
    "    os.chdir(\"data/output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate_dict = {\n",
    "    \"15min\" : \"15min\",\n",
    "    \"01h\" : \"01h\",\n",
    "    \"04h\" : \"04h\",\n",
    "    \"24h\" : \"24h\",\n",
    "    \"96h\" : \"96h\",\n",
    "    \"DirVento\" : \"WindDir\",\n",
    "    \"Pressao\" : \"Pressure\",\n",
    "    \"Temperatura\" : \"Temperature\",\n",
    "    \"Umidade\" : \"Humidity\",\n",
    "    \"VelVento\" : \"WindSpeed\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996bc910",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "\n",
    "In this section the dataset is loaded and unified as one pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb101efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "station = \"sao_cristovao\"\n",
    "sources = ['AlertaRio_DadosPluv', 'AlertaRio_DadosMet']\n",
    "data[station] = {}\n",
    "source = sources[0]\n",
    "data[station][source] = data[station][source] = pd.read_csv(source + \"/full/\" + station + \".csv\", sep=',')\n",
    "source = sources[1]\n",
    "data[station][source] = data[station][source] = pd.read_csv(source + \"/full/\" + station + \".csv\", sep=',')\n",
    "\n",
    "for source in sources:\n",
    "    data[station][source]['datetime'] = pd.to_datetime(data[station][source]['Dia'] + data[station][source]['Hora'], format='%d/%m/%Y%H:%M:%S')\n",
    "    data[station][source].set_index('datetime', inplace=True)\n",
    "    data[station][source] = data[station][source].asfreq(\"15T\")[\"2000\":\"2023-05-18 02:00:00\"]\n",
    "data[station][sources[1]].drop(columns=[\"Chuva\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64df8b",
   "metadata": {},
   "source": [
    "Checking for the same range of date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75272e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"sao_cristovao\"][\"AlertaRio_DadosPluv\"].iloc[0].name == \\\n",
    "data[\"sao_cristovao\"][\"AlertaRio_DadosMet\"].iloc[0].name, \\\n",
    "\\\n",
    "data[\"sao_cristovao\"][\"AlertaRio_DadosPluv\"].iloc[-1].name == \\\n",
    "data[\"sao_cristovao\"][\"AlertaRio_DadosMet\"].iloc[-1].name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926915f1",
   "metadata": {},
   "source": [
    "Concatenate the 2 datasets and remove useless columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a9eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['Dia', 'Hora']\n",
    "data_features = pd.concat([data[\"sao_cristovao\"][\"AlertaRio_DadosPluv\"].drop(columns=drop_list), data[\"sao_cristovao\"][\"AlertaRio_DadosMet\"].drop(columns=drop_list)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0804bf87",
   "metadata": {},
   "source": [
    "### Features extraction\n",
    "\n",
    "Extract the features from the date to run XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(df):\n",
    "    out = df.copy()\n",
    "    out[\"min\"] = out.index.minute\n",
    "    out[\"hour\"] = out.index.hour\n",
    "    out[\"day\"] = out.index.day\n",
    "    out[\"month\"] = out.index.month\n",
    "    out[\"year\"] = out.index.year\n",
    "    \n",
    "#     out['quarter'] = out.index.quarter\n",
    "#     out['dayofyear'] = out.index.dayofyear\n",
    "#     out['dayofmonth'] = out.index.day\n",
    "    \n",
    "#     out['weekofyear'] = out.index.isocalendar().week.astype(np.int64)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1b8823",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_features = get_features(data_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a7d47",
   "metadata": {},
   "source": [
    "### Target definition and group points\n",
    "\n",
    "In this section the target is defined and for each point, the data from the previous points is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b6e2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = '04h'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8fe2d1",
   "metadata": {},
   "source": [
    "Checking for missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed691ad0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_features[target].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c515a",
   "metadata": {},
   "source": [
    "Interpolating the missing data using a linear regression. And keeping only the data after 2002 as the São Cristóvão station doesn't provide meteorological data in the first 2 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00fb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features[target] = data_features[target].interpolate(method='linear')\n",
    "\n",
    "data_features['2002':][target].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96caa02f",
   "metadata": {},
   "source": [
    "Grouping the N previous points in each rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5876b380",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_colums_names(column_names, N):\n",
    "    column_names = list(column_names)\n",
    "    names = []\n",
    "    for i in range(N, 0, -1):\n",
    "        for name in column_names:\n",
    "            names.append(name + str(i))\n",
    "    names.extend(column_names)\n",
    "    return names\n",
    "\n",
    "data_features.reset_index(inplace=True)\n",
    "all_available_features = list(data_features.columns)\n",
    "\n",
    "N = 10 # Number of points to predict future\n",
    "data_multiple = data_features.copy()\n",
    "\n",
    "for i in range(1, N):\n",
    "    data_multiple = pd.concat([data_multiple.iloc[:-1].reset_index(drop=True), data_features.iloc[i:].reset_index(drop=True)], axis=1)\n",
    "\n",
    "data_multiple = pd.concat([data_multiple.iloc[:-1].reset_index(drop=True), data_features.iloc[N:].reset_index(drop=True)], axis=1)\n",
    "\n",
    "data_multiple.columns = get_colums_names(data_features.columns, N)\n",
    "\n",
    "data_multiple.drop(columns=[\"datetime\" + str(i) for i in range(1, N + 1)], inplace=True)\n",
    "data_multiple.set_index(\"datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a80cb",
   "metadata": {},
   "source": [
    "### Partitionning\n",
    "\n",
    "Partitionning the dataset in a training and a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586ccad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = '2010'\n",
    "date2 = '2018'\n",
    "\n",
    "train, test = data_multiple[date1:date2], data_multiple[str(int(date2) + 1):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40830d19",
   "metadata": {},
   "source": [
    "Make sure the partition are next to each other and not mixed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c131e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.Timedelta(\"15T\")\n",
    "train.index.max() + t == test.index.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76783190",
   "metadata": {},
   "source": [
    "Visualizing the partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b19b68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(15, 15), sharex=True)\n",
    "\n",
    "ax[0].set_title('Data Train/Test Split')\n",
    "ax[0].plot(train[target])\n",
    "ax[0].plot(test[target])\n",
    "ax[0].legend(['Training Set', 'Test Set'])\n",
    "\n",
    "ax[1].set_title('Training Data')\n",
    "ax[1].plot(train[target])\n",
    "ax[1].legend(['Training Set'])\n",
    "\n",
    "ax[2].set_title('Testing Data')\n",
    "ax[2].plot(test[target], color=\"orange\")\n",
    "ax[2].legend(['Test Set'])\n",
    "\n",
    "ax[0].set_xlim([train.index.min(), test.index.max()])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce83d120",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5), sharex=True)\n",
    "\n",
    "ax.set_title('Data Train/Test Split')\n",
    "ax.plot(train[target])\n",
    "ax.plot(test[target])\n",
    "ax.legend(['Training Set', 'Test Set'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513ab11e",
   "metadata": {},
   "source": [
    "## Training and evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb8b10",
   "metadata": {},
   "source": [
    "Defining the training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = data_multiple.columns\n",
    "\n",
    "training_features_list = ['15min', '01h', '04h', '24h', '96h', 'DirVento',\n",
    "       'VelVento', 'Temperatura', 'Pressao', 'Umidade', 'hour', 'day', 'month',\n",
    "       'year', 'quarter', 'dayofyear', 'dayofmonth', 'weekofyear']\n",
    "\n",
    "# Features to avoid for each point (If a feature like 15min is in this list,\n",
    "# then training feature will contains 15min1, ..., 15minN but not 15min)\n",
    "avoid_features = ['datetime', '15min', '01h', '04h', '24h', '96h', 'DirVento',\n",
    "       'VelVento', 'Temperatura', 'Pressao', 'Umidade']\n",
    "\n",
    "def is_training_feature(feature, training_features, avoid_features):\n",
    "    for training_feature in training_features:\n",
    "        if feature not in avoid_features and \\\n",
    "        training_feature == feature[:len(training_feature)] and \\\n",
    "        (feature[len(training_feature):].isnumeric() or feature[len(training_feature):] == \"\"):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "training_features = list(filter(lambda x : is_training_feature(x, training_features_list, avoid_features), all_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5213029",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2806c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "args = {\n",
    "    \"n_estimators\" : 500,\n",
    "    \"learning_rate\" : 0.01\n",
    "}\n",
    "\n",
    "# If you are using a nvidia GPU you can use it below\n",
    "if True:\n",
    "    reg = xgb.XGBRegressor(tree_method=\"gpu_hist\",\n",
    "                           **args)\n",
    "else:\n",
    "    reg = xgb.XGBRegressor(**args)\n",
    "\n",
    "t = time()\n",
    "\n",
    "reg.fit(train[training_features], train[target],\n",
    "        eval_set=[(train[training_features], train[target]), (test[training_features],test[target])],\n",
    "        verbose=50)\n",
    "print(time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40400508",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f912384b",
   "metadata": {},
   "source": [
    "Evaluating the model using the RSME metric of XGBoost score method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9437a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_train = reg.predict(train[training_features])\n",
    "preds_test = reg.predict(test[training_features])\n",
    "\n",
    "print(\"Training :\", reg.score(train[training_features], train[target]))\n",
    "print(\"Testing :\", reg.score(test[training_features], test[target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875e07d7",
   "metadata": {},
   "source": [
    "Plotting the prediction on the training and testing partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b14ca8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 1, figsize=(15, 30), sharex=True)\n",
    "\n",
    "ax[0].set_title('Training Data/predicted value')\n",
    "ax[0].plot(train[target], color='r', alpha=1)\n",
    "ax[0].plot(train.index, preds_train, color='b', alpha=0.5)\n",
    "ax[0].legend(['Training Set', 'Prediction'])\n",
    "\n",
    "ax[1].set_title('Training Data')\n",
    "ax[1].plot(train[target])\n",
    "ax[1].legend(['Training Set'])\n",
    "\n",
    "ax[2].set_title('Prediction value')\n",
    "ax[2].plot(train.index, preds_train, color=\"orange\")\n",
    "ax[2].legend(['Prediction'])\n",
    "\n",
    "ax[3].set_title('Testing Data/predicted value')\n",
    "ax[3].plot(test[target], alpha=0.7)\n",
    "ax[3].plot(test.index, preds_test, alpha=0.7)\n",
    "ax[3].legend(['Testing Set', 'Prediction'])\n",
    "\n",
    "ax[4].set_title('Testing Data')\n",
    "ax[4].plot(test[target])\n",
    "ax[4].legend(['Testing Set'])\n",
    "\n",
    "ax[5].set_title('Prediction value')\n",
    "ax[5].plot(test.index, preds_test, color=\"orange\")\n",
    "ax[5].legend(['Prediction'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754fecd7",
   "metadata": {},
   "source": [
    "Looking at the prediction during an extreme event (in the testing partition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eb3350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "period1 = '2019 04 08'\n",
    "period2 = '2019 04 10'\n",
    "\n",
    "preds_period = reg.predict(test.loc[period1:period2][training_features])\n",
    "\n",
    "plt.title(f'Testing Data/predicted value for feature {translate_dict[target]}')\n",
    "plt.plot(test.loc[period1:period2][target], color=\"blue\")\n",
    "plt.plot(test.loc[period1:period2].index, preds_period, alpha=0.7, color=\"red\")\n",
    "plt.legend(['Testing Set', 'Prediction'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe879c",
   "metadata": {},
   "source": [
    "Looking at the prediction during usual weather (in the testing partition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec790899",
   "metadata": {},
   "outputs": [],
   "source": [
    "period1 = '2019 04 14'\n",
    "period2 = '2019 04 20'\n",
    "\n",
    "preds_period = reg.predict(test.loc[period1:period2][training_features])\n",
    "\n",
    "plt.title(f'Testing Data/predicted value for feature {translate_dict[target]}')\n",
    "plt.plot(test.loc[period1:period2][target], color=\"blue\")\n",
    "plt.plot(test.loc[period1:period2].index, preds_period, alpha=0.7, color=\"red\")\n",
    "plt.legend(['Testing Set', 'Prediction'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d754ae",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b74654",
   "metadata": {},
   "source": [
    "The model is very good to predict one point into the future and predicting only rain. However, in order to predict multiple point in the future, we need the other features to be predicted.\n",
    "Indeed, our prediction are made using wind speed, wind direction, temperature, humidity, and pressure, therefore the model need all those features to predict.\n",
    "To fix this the next step in the experiment is to predict all the features using a XGBoost regressor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
